{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word2Vec & Doc2Vec with Gensim\n",
    "\n",
    "## Tim Hochberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Word2Vec\n",
    "\n",
    "* **Unsupervised** method of mapping words to a vector space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 750M (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Load pretrained model from https://github.com/3Top/word2vec-api/blob/master/GoogleNews-vectors-negative300.bin.gz\n",
    "model = Word2Vec.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model[\"car\"]) # Words map to 300 element vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04943277,  0.00318177,  0.01263487], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"car\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Similar words get mapped to nearby vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67357901840345347"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> model.n_similarity(['car'], ['truck'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.092147685651241318"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> model.n_similarity(['car'], ['fish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cows', 0.7792555093765259),\n",
       " ('pig', 0.6542098522186279),\n",
       " ('dairy_cow', 0.6442502737045288),\n",
       " ('bovines', 0.6437903642654419),\n",
       " ('bovine', 0.6407181620597839),\n",
       " ('goat', 0.6359611749649048),\n",
       " ('cattle', 0.631946325302124),\n",
       " ('sheep', 0.6081749200820923),\n",
       " ('Holstein_cow', 0.6066932678222656),\n",
       " ('goats', 0.5846577286720276)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> model.most_similar(['cow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Vector arithmetic solves analogy questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431607246399),\n",
       " ('crown_prince', 0.5499460697174072)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# man:king as woman:?\n",
    "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('iPhone', 0.6760426759719849),\n",
       " ('Android_OS', 0.6658217906951904),\n",
       " ('iOS', 0.6550344228744507),\n",
       " ('WP7', 0.6534616947174072)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Google:Android as Apple:?\n",
    "model.most_similar(positive=[\"Android\", \"Apple\"], negative=[\"Google\"], topn=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('forearm', 0.6154942512512207),\n",
       " ('arm', 0.5596616268157959),\n",
       " ('legs', 0.5394924879074097),\n",
       " ('puncturing_lung', 0.5146561861038208)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# knee:leg as elbow:?\n",
    "model.most_similar(positive=[\"leg\", \"elbow\"], negative=[\"knee\"], topn=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*More examples like this, but many sensible analogies fail*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How?\n",
    "\n",
    "* Words appearing in similar contexts mapped close together\n",
    "* word2vec is a family of related algorithms\n",
    "   - Continuous Bag of Words (CBOW) | Skip-Gram\n",
    "   - Hierarchical Softmax | Negative-Sampling\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![skip-gram](skip_gram.png)\n",
    "*From [word2vec Parameter Learning Explained](http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Words one-hot encoded (~1e6 words in English)\n",
    "* Network trained to predict context based on word\n",
    "* Hidden layer is the word vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to Train on Custom Corpus\n",
    "\n",
    "1. Convert corpus to list of 'sentences'\n",
    "   - each sentence is a list of 'words'\n",
    "   - 'words' words may be phrases or punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convert movie review data to correct form\n",
    "\n",
    "Data from [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review_text):\n",
    "    review_text = review_text.replace(\"<br />\", \" \")\n",
    "    for x in '\"()?.;:!,':\n",
    "        review_text = review_text.replace(x, \" \"+x+\" \")\n",
    "    return review_text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "base = \"aclImdb/\"\n",
    "\n",
    "train_sentences = []  \n",
    "train_sentiment = []\n",
    "\n",
    "for path in glob(base + \"train/pos/*.txt\"):\n",
    "    train_sentences.append(review_to_wordlist(open(path).read()))\n",
    "    train_sentiment.append(1)\n",
    "    \n",
    "for path in glob(base + \"train/neg/*.txt\"):\n",
    "    train_sentences.append(review_to_wordlist(open(path).read()))\n",
    "    train_sentiment.append(0)  \n",
    "    \n",
    "for path in glob(base + \"train/unsup/*.txt\"):\n",
    "    train_sentences.append(review_to_wordlist(open(path).read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sentences = []  \n",
    "test_sentiment = []\n",
    "\n",
    "for path in glob(base + \"test/pos/*.txt\"):\n",
    "    test_sentences.append(review_to_wordlist(open(path).read()))\n",
    "    test_sentiment.append(1)\n",
    "    \n",
    "for path in glob(base + \"test/neg/*.txt\"):\n",
    "    test_sentences.append(review_to_wordlist(open(path).read()))\n",
    "    test_sentiment.append(0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75000\n",
      "25000\n",
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', '.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'teachers', '\"', '.', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', \"high's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'teachers', '\"', '.', 'the', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', '.', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'i', 'immediately', 'recalled', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'at', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'high', '.', 'a', 'classic', 'line', ':', 'inspector', ':', \"i'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'student', ':', 'welcome', 'to', 'bromwell', 'high', '.', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', '.', 'what', 'a', 'pity', 'that', 'it', \"isn't\", '!']\n"
     ]
    }
   ],
   "source": [
    "# Check how many sentences we have in total \n",
    "print(len(train_sentences)) \n",
    "print(len(test_sentences))\n",
    "print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to Train on Custom Corpus\n",
    "\n",
    "1. Convert corpus to list of 'sentences'\n",
    "   - each sentence is a list of 'words'\n",
    "   - 'words' words may be phrases or punctuation\n",
    "2. Train \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec       \n",
    "\n",
    "movie_model = word2vec.Word2Vec(train_sentences, \n",
    "                                workers=6, # Number of threads to run in parallel\n",
    "                                size=100,  # Word vector dimensionality\n",
    "                                window=10  # Context window size \n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aliens', 0.7625104784965515),\n",
       " ('robot', 0.688758373260498),\n",
       " ('predator', 0.6847430467605591),\n",
       " ('mutant', 0.6433262825012207),\n",
       " ('planet', 0.6344485878944397),\n",
       " ('space', 0.6288543343544006),\n",
       " ('virus', 0.6277874708175659),\n",
       " ('attack', 0.6222692728042603),\n",
       " ('creature', 0.6197825074195862),\n",
       " ('monster', 0.617556095123291)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_model.most_similar(\"alien\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aliens', 0.786628246307373),\n",
       " ('extraterrestrial', 0.6394338607788086),\n",
       " ('extra_terrestrials', 0.6255451440811157),\n",
       " ('Klaatu_Keanu_Reeves', 0.6106236577033997),\n",
       " ('extraterrestrials', 0.60221266746521),\n",
       " ('intergalactic', 0.6010125875473022),\n",
       " ('humanoid_aliens', 0.6000779867172241),\n",
       " ('earthling', 0.5918036699295044),\n",
       " ('Alien', 0.5912981629371643),\n",
       " ('alien_invader', 0.5830426812171936)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"alien\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to Train on Custom Corpus\n",
    "\n",
    "1. Convert corpus to list of 'sentences'\n",
    "   - each sentence is a list of 'words'\n",
    "   - 'words' words may be phrases or punctuation\n",
    "2. Train \n",
    "3. **???**\n",
    "4. Profit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ???\n",
    "\n",
    "* Usually want to compare larger units of text\n",
    "* Word vectors can be used as features for higher level model\n",
    "   - Recurrent Neural Net (RNN)\n",
    "   - Bag of centroids\n",
    "   - Fixed length models\n",
    "   - ...\n",
    "* Ideally want equivalent to word2vec on chunks of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Doc2vec (aka Paragraph2vec)\n",
    "\n",
    "* doc2vec is a family of related algorithms (again)\n",
    "    - Distributed Memory (PV-DM) | Distributed Bag of Words (PV-DBOW)\n",
    "    - Hierarchical Softmax | Negative-Sampling \n",
    "    - Maps paragraphs/documents to vectors\n",
    "    - PV-DM also maps words to vectors\n",
    "* Can compare web pages, movie review, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![PV-DBOW](PV-DBOW.png)\n",
    "*From [Distributed Representations of Sentences and Documents](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import LabeledSentence, Doc2Vec\n",
    "import numpy as np\n",
    "\n",
    "# labels is a list of ids identifying which review a given sentence came from\n",
    "labelled_sentences = [LabeledSentence(words=s, \n",
    "                                      tags=['P{0}'.format(i)]) for (i,s) in enumerate(train_sentences)]\n",
    "\n",
    "def train(model, alpha=0.025, min_alpha=0.001, epochs=20):\n",
    "    model.build_vocab(labelled_sentences)\n",
    "    shuffled = labelled_sentences[:]\n",
    "    for a in np.linspace(alpha, min_alpha, epochs):\n",
    "        print(\"alpha =\", a)\n",
    "        np.random.shuffle(shuffled)\n",
    "        model.alpha = model.min_alpha = a\n",
    "        model.train(shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Train two networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.025\n",
      "alpha = 0.0237368421053\n",
      "alpha = 0.0224736842105\n",
      "alpha = 0.0212105263158\n",
      "alpha = 0.0199473684211\n",
      "alpha = 0.0186842105263\n",
      "alpha = 0.0174210526316\n",
      "alpha = 0.0161578947368\n",
      "alpha = 0.0148947368421\n",
      "alpha = 0.0136315789474\n",
      "alpha = 0.0123684210526\n",
      "alpha = 0.0111052631579\n",
      "alpha = 0.00984210526316\n",
      "alpha = 0.00857894736842\n",
      "alpha = 0.00731578947368\n",
      "alpha = 0.00605263157895\n",
      "alpha = 0.00478947368421\n",
      "alpha = 0.00352631578947\n",
      "alpha = 0.00226315789474\n",
      "alpha = 0.001\n"
     ]
    }
   ],
   "source": [
    "pv_dm_model = Doc2Vec(size=100, negative=10, window=5, min_count=2, dm=1, hs=0, dm_concat=0)\n",
    "train(pv_dm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.025\n",
      "alpha = 0.0237368421053\n",
      "alpha = 0.0224736842105\n",
      "alpha = 0.0212105263158\n",
      "alpha = 0.0199473684211\n",
      "alpha = 0.0186842105263\n",
      "alpha = 0.0174210526316\n",
      "alpha = 0.0161578947368\n",
      "alpha = 0.0148947368421\n",
      "alpha = 0.0136315789474\n",
      "alpha = 0.0123684210526\n",
      "alpha = 0.0111052631579\n",
      "alpha = 0.00984210526316\n",
      "alpha = 0.00857894736842\n",
      "alpha = 0.00731578947368\n",
      "alpha = 0.00605263157895\n",
      "alpha = 0.00478947368421\n",
      "alpha = 0.00352631578947\n",
      "alpha = 0.00226315789474\n",
      "alpha = 0.001\n"
     ]
    }
   ],
   "source": [
    "pv_dbow_model = Doc2Vec(size=100, negative=10, window=10, min_count=2, hs=0, dm=0)\n",
    "train(pv_dbow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generate Features\n",
    "\n",
    "*Note that we **infer** vectors for the test features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "n_labeled = len(train_sentiment)\n",
    "\n",
    "train_features = np.array([np.concatenate([pv_dm_model.docvecs['P{0}'.format(i)], \n",
    "                                           pv_dbow_model.docvecs['P{0}'.format(i)]]\n",
    "                                         ) for i in range(n_labeled) ])\n",
    "train_labels = np.array(train_sentiment)\n",
    "\n",
    "test_features = np.array([np.concatenate([pv_dm_model.infer_vector(x),\n",
    "                                          pv_dbow_model.infer_vector(x)\n",
    "                                         ]) for x in test_sentences])\n",
    "\n",
    "test_labels = np.array(test_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Train a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "classifier.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11524\n"
     ]
    }
   ],
   "source": [
    "predicted = classifier.predict(test_features)\n",
    "accuracy = (test_labels == predicted).mean()\n",
    "\n",
    "# Paragraph2Vec Paper: 7.42%\n",
    "# Doc2Vec Example: 9.48%\n",
    "print(1 - accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Links\n",
    "\n",
    "* Original word2vec paper: http://www.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf\n",
    "* word2vec Parameter Learning Explained: http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf\n",
    "* Distributed Representations of Sentences and Documents: https://cs.stanford.edu/~quocle/paragraph_vector.pdf\n",
    "* Gensim: https://radimrehurek.com/gensim/\n",
    "* Kaggle word2vec tutorial: https://www.kaggle.com/c/word2vec-nlp-tutorial\n",
    "* piskvorky IMDB Doc2Vec example: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
